--- Pods ----------------------------------------------------
# create a pod/replcationcontroller/replicaset
kubectl create -f pod-definition.yml

# get running node/pod/replcationcontroller/replicaset
kubectl get pods

# delete a running pod/replcationcontroller/replicaset with name
kubectl delete pod <<pod_name>>

# get info about a new pod
kubectl describe pod <<pod_name>>

# edit pod config
kubectl edit pod <<pod_name>>

# check pods in detail
kubectl get pods --all-namespaces -o wide

# check logs of a container in an app/pod
kubectl logs webapp-2 -c simple-webapp

# check logs of a pod at a particular namespace
kubectl -n elastic-stack exec -it app cat /log/app.log

# get inside an app 
kubectl -n elastic-stack exec -it app cat /log/app.log

# get existing pod config as yaml/file
kubectl get pod app -n elastic-stack -o yaml > app.yaml

# who is running the pod
kubectl exec ubuntu-sleeper -- whoami

# find pods by label
kubectl get pods -l name=payroll

# show labels
kubectl get pods --show-labels

# to get the image used by a scheduler pod
kubectl -n kube-system describe pod kube-sceduler-master | grep -i image

--- Replicaset ----------------------------------------------------
# scale the replicaset with more replica
kubectl replace -f replicaset-definition.yml

# another way
kubectl scale --replicas=6 -f replicaset-definition.yml

# get all replicasets
kubectl get replicasets.apps

--- Deployments ----------------------------------------------------
# get all deployments
kubectl get deployments.apps

# get image name
kubectl describe deployments.apps frontend-deployment | grep -i image

# change deployment file and apply
kubectl apply -f deployment-definition.yml

# one line deployment create/edit changes
kubectl create deployment httpd-frontend --image=httpd:2.4-alpine
kubectl scale deployment httpd-frontend --replicas=3 
kubectl expose deployment nginx --port 80

# change image for a deployment
kubectl set image deployment nginx nginx=nginx:1.18

# get status
kubectl get deployments.apps httpd-frontend

# dry run deployment to create a file
kubectl create deployment <<deployment_name>> --image=nginx --dry-run -o yaml > red.yaml

# rollout - status/history
kubectl rollout status deployment nginx
kubectl rollout history deployment nginx
kubectl rollout history deployment nginx --revision=1
kubectl set image deployment nginx nginx=nginx:1.17 --record

--- Namespaces ----------------------------------------------------
# create namespace
kubectl create namespace <<namespace_name>>

# get pods for a specific namespace
kubectl get pods --namespace=prod

# get pods on a specific node 
kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=node01

# set context for a namespace
kubectl config set-context $(kubectl config current-context) --namespace=dev

# create pod in namespace
kubectl run redis --image=redis --namespace=finance

# find a pod
kubectl get pods --all-namespaces | grep blue

# expose a running pod
kubectl expose pod nginx --port 80 --name=nginx-service

# get namespaces (wc -l count number of lines and --no-headers avoid header)
kubectl get namespace --no-headers | wc -l

# get pods in a namespace
kubectl get pods --namespace=<<namespace_name>>

--- Services ----------------------------------------------------
# create a service
kubectl create -f service-definition.yml

# get services
kubectl get services/svc

# describe services (describe <<type>> <<name>>)
kubectl describe svc kubernetes

# expose a deployment to a file
kubectl expose deployment <<deployment_name>> --name=<<deployment_short_name>> --target-port=<<port_number>> --type=NodePort --port=<<port_number>> --dry-run=client -o yaml > <<deployment_file>>

# change and apply a service <-- declarative
kubectl apply -f <<deployment_file>>

# create a file on the fly
kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis-pod.yaml

# create a file on the fly 2: check file creation first look (whenever you're creating a pod and exposing it, it automatically creates a service, cause you can expose a service not a pod)
kubectl run httpd --image=httpd:alpine --port 80 --expose --dry-run=client -o yaml

# get pods by by selector
kubectl get pods --selector env=dev
kubectl get pods --selector bu=finance

# get all info including replicas
kubectl get all --selector env=prod

--- Taints ----------------------------------------------------
# taint a node
kubectl taint node node01 spray=mortein:NoSchedule

# un-taint a node (kubectl taint nodes node1 key1=value1:NoSchedule-)
kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

# label a node
kubectl label node node01 color=blue

# create deployment with specification
kubectl create deployment blue --image=nginx --replicas=3

# get taints associated with a node
kubectl describe node node01 | grep -iF taints

--- Daemonsets ----------------------------------------------------
# get all DaemonSets in all namespaces
kubectl get DaemonSets --all-namespaces

# create a DaemonSet
# by create a deployment, remove strategy, replicas, timeouts, change kind -> deployment to daemonset
kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml

--- Special -------------------------------------------------------
# /etc/kubernetes/manifests (if not then check the kubelet config) <-- path where static pods are located <-- create a file there with image, pod name and command
kubectl run static-busybox --restart=Never --image=busybox --dry-run=client -o yaml --command sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
ps -ef | grep /usr/bin/kubelet

# ssh to another node
ssh <<node_name>>

# drain node for maintenance
kubectl drain <node_name> --ignore-daemonsets

# drain node for maintenance forcefully
kubectl drain <node_name> --ignore-daemonsets --force

# cordon/uncordon a node (mark a node as unschedulable/schedulable)
kubectl uncordon node01

# get all configs used to create all the pods
kubectl get all --all-namespaces -o -yaml > all-config.yaml

# backup etcd
etcdctl snapshot save snapshot.db
service kube-apiserver stop
etcdctl restore snapshot.db --data-dir /var/lib/etcd-from-backup
systemctl daemon-reload
service etcd restart

# get etcd version from namespace kube-system <-- always in the namespace kube-system and master/controlplane node
kubectl describe pod etcd-controlplane -n kube-system

# address to reach etcd cluster
kubectl describe pod etcd-controlplane -n kube-system | grep -i listen-client-urls

--- Ingress --------------------------------------------------------
# create ingress
kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

# get all ingresses
kubectl get ingress --all-namespaces

--- Configs and secrets --------------------------------------------
# create a configmap
kubectl create configmap webapp-color --from-literal=APP_COLOR=darkblue --dry-run=client -o yaml > webapp-color-configmap.yaml

# create a secret
kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123 --dry-run=client -o yaml > db-secret.yaml

